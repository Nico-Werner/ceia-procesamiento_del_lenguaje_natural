{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: numpy in /opt/anaconda3/envs/Posgrado-AI/lib/python3.12/site-packages (1.26.4)\n",
            "Requirement already satisfied: scikit-learn in /opt/anaconda3/envs/Posgrado-AI/lib/python3.12/site-packages (1.5.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /opt/anaconda3/envs/Posgrado-AI/lib/python3.12/site-packages (from scikit-learn) (1.14.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /opt/anaconda3/envs/Posgrado-AI/lib/python3.12/site-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/anaconda3/envs/Posgrado-AI/lib/python3.12/site-packages (from scikit-learn) (3.5.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install numpy scikit-learn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zq6j8LsYq1Dr"
      },
      "source": [
        "### Vectorización de texto y modelo de clasificación Naïve Bayes con el dataset 20 newsgroups"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "l7cXR6CI30ry"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.naive_bayes import MultinomialNB, ComplementNB\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# 20newsgroups por ser un dataset clásico de NLP ya viene incluido y formateado\n",
        "# en sklearn\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yD-pVDWV_rQc"
      },
      "source": [
        "## Carga de datos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Ech9qJaUo9vK"
      },
      "outputs": [],
      "source": [
        "# cargamos los datos (ya separados de forma predeterminada en train y test)\n",
        "newsgroups_train = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'))\n",
        "newsgroups_test = fetch_20newsgroups(subset='test', remove=('headers', 'footers', 'quotes'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UxjSI7su_uWI"
      },
      "source": [
        "## Vectorización"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "-94VP0QYCzDn"
      },
      "outputs": [],
      "source": [
        "# instanciamos un vectorizador\n",
        "# ver diferentes parámetros de instanciación en la documentación de sklearn https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\n",
        "tfidfvect = TfidfVectorizer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "id": "ftPlyanuak8n",
        "outputId": "45a94d0e-49e7-4f7c-c806-7d5b66779dd0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "I was wondering if anyone out there could enlighten me on this car I saw\n",
            "the other day. It was a 2-door sports car, looked to be from the late 60s/\n",
            "early 70s. It was called a Bricklin. The doors were really small. In addition,\n",
            "the front bumper was separate from the rest of the body. This is \n",
            "all I know. If anyone can tellme a model name, engine specs, years\n",
            "of production, where this car is made, history, or whatever info you\n",
            "have on this funky looking car, please e-mail.\n"
          ]
        }
      ],
      "source": [
        "# en el atributo `data` accedemos al texto\n",
        "print(newsgroups_train.data[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "1zxcXV6aC_oL"
      },
      "outputs": [],
      "source": [
        "# con la interfaz habitual de sklearn podemos fitear el vectorizador\n",
        "# (obtener el vocabulario y calcular el vector IDF)\n",
        "# y transformar directamente los datos\n",
        "X_train = tfidfvect.fit_transform(newsgroups_train.data)\n",
        "# `X_train` la podemos denominar como la matriz documento-término"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Sv7TXbda41-",
        "outputId": "dcca5de6-dac1-4d68-d284-ce7be2ed9e2f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'scipy.sparse._csr.csr_matrix'>\n",
            "shape: (11314, 101631)\n",
            "Cantidad de documentos: 11314\n",
            "Tamaño del vocabulario (dimensionalidad de los vectores): 101631\n"
          ]
        }
      ],
      "source": [
        "# recordar que las vectorizaciones por conteos son esparsas\n",
        "# por ello sklearn convenientemente devuelve los vectores de documentos\n",
        "# como matrices esparsas\n",
        "print(type(X_train))\n",
        "print(f'shape: {X_train.shape}')\n",
        "print(f'Cantidad de documentos: {X_train.shape[0]}')\n",
        "print(f'Tamaño del vocabulario (dimensionalidad de los vectores): {X_train.shape[1]}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dgydNTZ2pAgR",
        "outputId": "95111464-e40c-4b57-d154-ede153739a82"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "25775"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# una vez fiteado el vectorizador, podemos acceder a atributos como el vocabulario\n",
        "# aprendido. Es un diccionario que va de términos a índices.\n",
        "# El índice es la posición en el vector de documento.\n",
        "tfidfvect.vocabulary_['car']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "xnTSZuvyrTcP"
      },
      "outputs": [],
      "source": [
        "# es muy útil tener el diccionario opuesto que va de índices a términos\n",
        "idx2word = {v: k for k,v in tfidfvect.vocabulary_.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "swa-AgWrMSHM",
        "outputId": "93d09f31-4c42-4215-e750-a13c83ecf96a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([ 7,  4,  4,  1, 14, 16, 13,  3,  2,  4])"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# en `y_train` guardamos los targets que son enteros\n",
        "y_train = newsgroups_train.target\n",
        "y_train[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "je5kxvQMDLvf",
        "outputId": "59799046-9799-406f-c4be-81c5765de58d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "clases [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['alt.atheism',\n",
              " 'comp.graphics',\n",
              " 'comp.os.ms-windows.misc',\n",
              " 'comp.sys.ibm.pc.hardware',\n",
              " 'comp.sys.mac.hardware',\n",
              " 'comp.windows.x',\n",
              " 'misc.forsale',\n",
              " 'rec.autos',\n",
              " 'rec.motorcycles',\n",
              " 'rec.sport.baseball',\n",
              " 'rec.sport.hockey',\n",
              " 'sci.crypt',\n",
              " 'sci.electronics',\n",
              " 'sci.med',\n",
              " 'sci.space',\n",
              " 'soc.religion.christian',\n",
              " 'talk.politics.guns',\n",
              " 'talk.politics.mideast',\n",
              " 'talk.politics.misc',\n",
              " 'talk.religion.misc']"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# hay 20 clases correspondientes a los 20 grupos de noticias\n",
        "print(f'clases {np.unique(newsgroups_test.target)}')\n",
        "newsgroups_test.target_names"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXCICFSd_y90"
      },
      "source": [
        "## Similaridad de documentos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_pki_olShnyE",
        "outputId": "b2bd8485-40c7-4923-c8a9-4ad6b735576e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "THE WHITE HOUSE\n",
            "\n",
            "                  Office of the Press Secretary\n",
            "                   (Pittsburgh, Pennslyvania)\n",
            "______________________________________________________________\n",
            "For Immediate Release                         April 17, 1993     \n",
            "\n",
            "             \n",
            "                  RADIO ADDRESS TO THE NATION \n",
            "                        BY THE PRESIDENT\n",
            "             \n",
            "                Pittsburgh International Airport\n",
            "                    Pittsburgh, Pennsylvania\n",
            "             \n",
            "             \n",
            "10:06 A.M. EDT\n",
            "             \n",
            "             \n",
            "             THE PRESIDENT:  Good morning.  My voice is coming to\n",
            "you this morning through the facilities of the oldest radio\n",
            "station in America, KDKA in Pittsburgh.  I'm visiting the city to\n",
            "meet personally with citizens here to discuss my plans for jobs,\n",
            "health care and the economy.  But I wanted first to do my weekly\n",
            "broadcast with the American people. \n",
            "             \n",
            "             I'm told this station first broadcast in 1920 when\n",
            "it reported that year's presidential elections.  Over the past\n",
            "seven decades presidents have found ways to keep in touch with\n",
            "the people, from whistle-stop tours to fire-side chats to the bus\n",
            "tour that I adopted, along with Vice President Gore, in last\n",
            "year's campaign.\n",
            "             \n",
            "             Every Saturday morning I take this time to talk with\n",
            "you, my fellow Americans, about the problems on your minds and\n",
            "what I'm doing to try and solve them.  It's my way of reporting\n",
            "to you and of giving you a way to hold me accountable.\n",
            "             \n",
            "             You sent me to Washington to get our government and\n",
            "economy moving after years of paralysis and policy and a bad\n",
            "experiment with trickle-down economics.  You know how important\n",
            "it is for us to make bold, comprehensive changes in the way we do\n",
            "business.  \n",
            "             \n",
            "             We live in a competitive global economy.  Nations\n",
            "rise and fall on the skills of their workers, the competitiveness\n",
            "of their companies, the imagination of their industries, and the\n",
            "cooperative experience and spirit that exists between business,\n",
            "labor and government.  Although many of the economies of the\n",
            "industrialized world are now suffering from slow growth, they've\n",
            "made many of the smart investments and the tough choices which\n",
            "our government has for too long ignored.  That's why many of them\n",
            "have been moving ahead and too many of our people have been\n",
            "falling behind.\n",
            "             \n",
            "             We have an economy today that even when it grows is\n",
            "not producing new jobs.  We've increased the debt of our nation\n",
            "by four times over the last 12 years, and we don't have much to\n",
            "show for it.  We know that wages of most working people have\n",
            "stopped rising, that most people are working longer work weeks\n",
            "and that too many families can no longer afford the escalating\n",
            "cost of health care.\n",
            "             \n",
            "             But we also know that, given the right tools, the\n",
            "right incentives and the right encouragement, our workers and\n",
            "businesses can make the kinds of products and profits our economy\n",
            "needs to expand opportunity and to make our communities better\n",
            "places to live.\n",
            "             \n",
            "             In many critical products today Americans are the\n",
            "low cost, high quality producers.  Our task is to make sure that\n",
            "we create more of those kinds of jobs.\n",
            "             \n",
            "             Just two months ago I gave Congress my plan for\n",
            "long-term jobs and economic growth.  It changes the old\n",
            "priorities in Washington and puts our emphasis where it needs to\n",
            "be -- on people's real needs, on increasing investments and jobs\n",
            "and education, on cutting the federal deficit, on stopping the\n",
            "waste which pays no dividends, and redirecting our precious\n",
            "resources toward investment that creates jobs now and lays the\n",
            "groundwork for robust economic growth in the future.\n",
            "             \n",
            "             These new directions passed the Congress in record\n",
            "time and created a new sense of hope and opportunity in our\n",
            "country.  Then the jobs plan I presented to Congress, which would\n",
            "create hundreds of thousands of jobs, most of them in the private\n",
            "sector in 1993 and 1994, passed the House of Representatives.  It\n",
            "now has the support of a majority of the United States Senate. \n",
            "But it's been held up by a filibuster of a minority in the\n",
            "Senate, just 43 senators.  They blocked a vote that they know\n",
            "would result in the passage of our bill and the creation of jobs.\n",
            "             \n",
            "             The issue isn't politics; the issue is people. \n",
            "Millions of Americans are waiting for this legislation and\n",
            "counting on it, counting on us in Washington.  But the jobs bill\n",
            "has been grounded by gridlock.  \n",
            "             \n",
            "             I know the American people are tired of business as\n",
            "usual and politics as usual.  I know they don't want us to spin\n",
            "or wheels.  They want the recovery to get moving.  So I have\n",
            "taken a first step to break this gridlock and gone the extra\n",
            "mile.  Yesterday I offered to cut the size of this plan by 25\n",
            "percent -- from $16 billion to $12 billion.  \n",
            "             \n",
            "             It's not what I'd hoped for.  With 16 million\n",
            "Americans looking for full-time work, I simply can't let the bill\n",
            "languish when I know that even a compromise bill will mean\n",
            "hundreds of thousands of jobs for our people.  The mandate is to\n",
            "act to achieve change and move the country forward.  By taking\n",
            "this initiative in the face of an unrelenting Senate talkathon, I\n",
            "think we can respond to your mandate and achieve a significant\n",
            "portion of our original goals.\n",
            "             \n",
            "             First, we want to keep the programs as much as\n",
            "possible that are needed to generate jobs and meet human needs,\n",
            "including highway and road construction, summer jobs for young\n",
            "people, immunization for children, construction of waste water\n",
            "sites, and aid to small businesses.  We also want to keep funding\n",
            "for extended unemployment compensation benefits, for people who\n",
            "have been unemployed for a long time because the economy isn't\n",
            "creating jobs.\n",
            "             \n",
            "             Second, I've recommended that all the other programs\n",
            "in the bill be cut across-the-board by a little more than 40\n",
            "percent.\n",
            "             \n",
            "             And third, I've recommended a new element in this\n",
            "program to help us immediately start our attempt to fight against\n",
            "crime by providing $200 million for cities and towns to rehire\n",
            "police officers who lost their jobs during the recession and put\n",
            "them back to work protecting our people.  I'm also going to fight\n",
            "for a tough crime bill because the people of this country need it\n",
            "and deserve it.\n",
            "             \n",
            "             Now, the people who are filibustering this bill --\n",
            "the Republican senators -- say they won't vote for it because it\n",
            "increases deficit spending, because there's extra spending this\n",
            "year that hasn't already been approved.  That sounds reasonable,\n",
            "doesn't it?  Here's what they don't say.  This program is more\n",
            "than paid for by budget cuts over my five-year budget, and this\n",
            "budget is well within the spending limits already approved by the\n",
            "Congress this year.\n",
            "             \n",
            "             It's amazing to me that many of these same senators\n",
            "who are filibustering the bill voted during the previous\n",
            "administration for billions of dollars of the same kind of\n",
            "emergency spending, and much of it was not designed to put the\n",
            "American people to work.  \n",
            "             \n",
            "             This is not about deficit spending.  We have offered\n",
            "a plan to cut the deficit.  This is about where your priorities\n",
            "are -- on people or on politics.  \n",
            "             \n",
            "             Keep in mind that our jobs bill is paid for dollar\n",
            "for dollar.  It is paid for by budget cuts.  And it's the\n",
            "soundest investment we can now make for ourselves and our\n",
            "children.  I urge all Americans to take another look at this jobs\n",
            "and investment program; to consider again the benefits for all of\n",
            "us when we've helped make more American partners working to\n",
            "ensure the future of our nation and the strength of our economy.\n",
            "             \n",
            "             You know, if every American who wanted a job had\n",
            "one, we wouldn't have a lot of the other problems we have in this\n",
            "country today.  This bill is not a miracle, it's a modest first\n",
            "step to try to set off a job creation explosion in this country\n",
            "again.  But it's a step we ought to take.  And it is fully paid\n",
            "for over the life of our budget.\n",
            "             \n",
            "             Tell your lawmakers what you think.  Tell them how\n",
            "important the bill is.  If it passes, we'll all be winners.\n",
            "             \n",
            "             Good morning, and thank you for listening.\n"
          ]
        }
      ],
      "source": [
        "# Veamos similaridad de documentos. Tomemos algún documento\n",
        "idx = 4811\n",
        "print(newsgroups_train.data[idx])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "Ssa9bqJ-hA_v"
      },
      "outputs": [],
      "source": [
        "# midamos la similaridad coseno con todos los documentos de train\n",
        "cossim = cosine_similarity(X_train[idx], X_train)[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p_mDA7p3AzcQ",
        "outputId": "747a3923-4b1c-4e2b-921d-4ebf2b271b00"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([1.        , 0.70930477, 0.67474953, ..., 0.        , 0.        ,\n",
              "       0.        ])"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# podemos ver los valores de similaridad ordenados de mayor a menos\n",
        "np.sort(cossim)[::-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0OIhDA1jAryX",
        "outputId": "04ddf3ca-0741-42d7-8bbb-00bb395f7834"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([ 4811,  6635,  4253, ...,  1534, 10055,  4750])"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# y a qué documentos corresponden\n",
        "np.argsort(cossim)[::-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "hP7qLS4ZBLps"
      },
      "outputs": [],
      "source": [
        "# los 5 documentos más similares:\n",
        "mostsim = np.argsort(cossim)[::-1][1:6]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "QdJLHPJACvaj",
        "outputId": "926186cf-7d4c-4bd3-927b-ad00bf7f24f9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'talk.politics.misc'"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# el documento original pertenece a la clase:\n",
        "newsgroups_train.target_names[y_train[idx]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RWy_73epCbFG",
        "outputId": "daf534e5-b2a8-43d4-d05a-9c52b816cf69"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "talk.politics.misc\n",
            "talk.politics.misc\n",
            "talk.politics.misc\n",
            "talk.politics.misc\n",
            "talk.politics.misc\n"
          ]
        }
      ],
      "source": [
        "# y los 5 más similares son de las clases:\n",
        "for i in mostsim:\n",
        "  print(newsgroups_train.target_names[y_train[i]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zRoNnKwhBqzq"
      },
      "source": [
        "### Modelo de clasificación Naïve Bayes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "TPM0thDaLk0R",
        "outputId": "bc7fdc3e-d912-4e0c-9d9e-33efc97b46fc"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<style>#sk-container-id-1 {\n",
              "  /* Definition of color scheme common for light and dark mode */\n",
              "  --sklearn-color-text: black;\n",
              "  --sklearn-color-line: gray;\n",
              "  /* Definition of color scheme for unfitted estimators */\n",
              "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
              "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
              "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
              "  --sklearn-color-unfitted-level-3: chocolate;\n",
              "  /* Definition of color scheme for fitted estimators */\n",
              "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
              "  --sklearn-color-fitted-level-1: #d4ebff;\n",
              "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
              "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
              "\n",
              "  /* Specific color for light theme */\n",
              "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
              "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
              "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
              "  --sklearn-color-icon: #696969;\n",
              "\n",
              "  @media (prefers-color-scheme: dark) {\n",
              "    /* Redefinition of color scheme for dark theme */\n",
              "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
              "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
              "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
              "    --sklearn-color-icon: #878787;\n",
              "  }\n",
              "}\n",
              "\n",
              "#sk-container-id-1 {\n",
              "  color: var(--sklearn-color-text);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 pre {\n",
              "  padding: 0;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 input.sk-hidden--visually {\n",
              "  border: 0;\n",
              "  clip: rect(1px 1px 1px 1px);\n",
              "  clip: rect(1px, 1px, 1px, 1px);\n",
              "  height: 1px;\n",
              "  margin: -1px;\n",
              "  overflow: hidden;\n",
              "  padding: 0;\n",
              "  position: absolute;\n",
              "  width: 1px;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-dashed-wrapped {\n",
              "  border: 1px dashed var(--sklearn-color-line);\n",
              "  margin: 0 0.4em 0.5em 0.4em;\n",
              "  box-sizing: border-box;\n",
              "  padding-bottom: 0.4em;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-container {\n",
              "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
              "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
              "     so we also need the `!important` here to be able to override the\n",
              "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
              "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
              "  display: inline-block !important;\n",
              "  position: relative;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-text-repr-fallback {\n",
              "  display: none;\n",
              "}\n",
              "\n",
              "div.sk-parallel-item,\n",
              "div.sk-serial,\n",
              "div.sk-item {\n",
              "  /* draw centered vertical line to link estimators */\n",
              "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
              "  background-size: 2px 100%;\n",
              "  background-repeat: no-repeat;\n",
              "  background-position: center center;\n",
              "}\n",
              "\n",
              "/* Parallel-specific style estimator block */\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item::after {\n",
              "  content: \"\";\n",
              "  width: 100%;\n",
              "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
              "  flex-grow: 1;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel {\n",
              "  display: flex;\n",
              "  align-items: stretch;\n",
              "  justify-content: center;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  position: relative;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
              "  align-self: flex-end;\n",
              "  width: 50%;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
              "  align-self: flex-start;\n",
              "  width: 50%;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
              "  width: 0;\n",
              "}\n",
              "\n",
              "/* Serial-specific style estimator block */\n",
              "\n",
              "#sk-container-id-1 div.sk-serial {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "  align-items: center;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  padding-right: 1em;\n",
              "  padding-left: 1em;\n",
              "}\n",
              "\n",
              "\n",
              "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
              "clickable and can be expanded/collapsed.\n",
              "- Pipeline and ColumnTransformer use this feature and define the default style\n",
              "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
              "*/\n",
              "\n",
              "/* Pipeline and ColumnTransformer style (default) */\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable {\n",
              "  /* Default theme specific background. It is overwritten whether we have a\n",
              "  specific estimator or a Pipeline/ColumnTransformer */\n",
              "  background-color: var(--sklearn-color-background);\n",
              "}\n",
              "\n",
              "/* Toggleable label */\n",
              "#sk-container-id-1 label.sk-toggleable__label {\n",
              "  cursor: pointer;\n",
              "  display: block;\n",
              "  width: 100%;\n",
              "  margin-bottom: 0;\n",
              "  padding: 0.5em;\n",
              "  box-sizing: border-box;\n",
              "  text-align: center;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
              "  /* Arrow on the left of the label */\n",
              "  content: \"▸\";\n",
              "  float: left;\n",
              "  margin-right: 0.25em;\n",
              "  color: var(--sklearn-color-icon);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
              "  color: var(--sklearn-color-text);\n",
              "}\n",
              "\n",
              "/* Toggleable content - dropdown */\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content {\n",
              "  max-height: 0;\n",
              "  max-width: 0;\n",
              "  overflow: hidden;\n",
              "  text-align: left;\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content pre {\n",
              "  margin: 0.2em;\n",
              "  border-radius: 0.25em;\n",
              "  color: var(--sklearn-color-text);\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
              "  /* Expand drop-down */\n",
              "  max-height: 200px;\n",
              "  max-width: 100%;\n",
              "  overflow: auto;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
              "  content: \"▾\";\n",
              "}\n",
              "\n",
              "/* Pipeline/ColumnTransformer-specific style */\n",
              "\n",
              "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Estimator-specific style */\n",
              "\n",
              "/* Colorize estimator box */\n",
              "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
              "#sk-container-id-1 div.sk-label label {\n",
              "  /* The background is the default theme color */\n",
              "  color: var(--sklearn-color-text-on-default-background);\n",
              "}\n",
              "\n",
              "/* On hover, darken the color of the background */\n",
              "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "/* Label box, darken color on hover, fitted */\n",
              "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Estimator label */\n",
              "\n",
              "#sk-container-id-1 div.sk-label label {\n",
              "  font-family: monospace;\n",
              "  font-weight: bold;\n",
              "  display: inline-block;\n",
              "  line-height: 1.2em;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-label-container {\n",
              "  text-align: center;\n",
              "}\n",
              "\n",
              "/* Estimator-specific */\n",
              "#sk-container-id-1 div.sk-estimator {\n",
              "  font-family: monospace;\n",
              "  border: 1px dotted var(--sklearn-color-border-box);\n",
              "  border-radius: 0.25em;\n",
              "  box-sizing: border-box;\n",
              "  margin-bottom: 0.5em;\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-estimator.fitted {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "/* on hover */\n",
              "#sk-container-id-1 div.sk-estimator:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
              "\n",
              "/* Common style for \"i\" and \"?\" */\n",
              "\n",
              ".sk-estimator-doc-link,\n",
              "a:link.sk-estimator-doc-link,\n",
              "a:visited.sk-estimator-doc-link {\n",
              "  float: right;\n",
              "  font-size: smaller;\n",
              "  line-height: 1em;\n",
              "  font-family: monospace;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  border-radius: 1em;\n",
              "  height: 1em;\n",
              "  width: 1em;\n",
              "  text-decoration: none !important;\n",
              "  margin-left: 1ex;\n",
              "  /* unfitted */\n",
              "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-unfitted-level-1);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link.fitted,\n",
              "a:link.sk-estimator-doc-link.fitted,\n",
              "a:visited.sk-estimator-doc-link.fitted {\n",
              "  /* fitted */\n",
              "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-fitted-level-1);\n",
              "}\n",
              "\n",
              "/* On hover */\n",
              "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
              ".sk-estimator-doc-link:hover,\n",
              "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
              ".sk-estimator-doc-link:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
              ".sk-estimator-doc-link.fitted:hover,\n",
              "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
              ".sk-estimator-doc-link.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "/* Span, style for the box shown on hovering the info icon */\n",
              ".sk-estimator-doc-link span {\n",
              "  display: none;\n",
              "  z-index: 9999;\n",
              "  position: relative;\n",
              "  font-weight: normal;\n",
              "  right: .2ex;\n",
              "  padding: .5ex;\n",
              "  margin: .5ex;\n",
              "  width: min-content;\n",
              "  min-width: 20ex;\n",
              "  max-width: 50ex;\n",
              "  color: var(--sklearn-color-text);\n",
              "  box-shadow: 2pt 2pt 4pt #999;\n",
              "  /* unfitted */\n",
              "  background: var(--sklearn-color-unfitted-level-0);\n",
              "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link.fitted span {\n",
              "  /* fitted */\n",
              "  background: var(--sklearn-color-fitted-level-0);\n",
              "  border: var(--sklearn-color-fitted-level-3);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link:hover span {\n",
              "  display: block;\n",
              "}\n",
              "\n",
              "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
              "\n",
              "#sk-container-id-1 a.estimator_doc_link {\n",
              "  float: right;\n",
              "  font-size: 1rem;\n",
              "  line-height: 1em;\n",
              "  font-family: monospace;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  border-radius: 1rem;\n",
              "  height: 1rem;\n",
              "  width: 1rem;\n",
              "  text-decoration: none;\n",
              "  /* unfitted */\n",
              "  color: var(--sklearn-color-unfitted-level-1);\n",
              "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
              "  /* fitted */\n",
              "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-fitted-level-1);\n",
              "}\n",
              "\n",
              "/* On hover */\n",
              "#sk-container-id-1 a.estimator_doc_link:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-3);\n",
              "}\n",
              "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MultinomialNB()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;MultinomialNB<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.naive_bayes.MultinomialNB.html\">?<span>Documentation for MultinomialNB</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>MultinomialNB()</pre></div> </div></div></div></div>"
            ],
            "text/plain": [
              "MultinomialNB()"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# es muy fácil instanciar un modelo de clasificación Naïve Bayes y entrenarlo con sklearn\n",
        "clf = MultinomialNB()\n",
        "clf.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "NrQjzM48Mu4T"
      },
      "outputs": [],
      "source": [
        "# con nuestro vectorizador ya fiteado en train, vectorizamos los textos\n",
        "# del conjunto de test\n",
        "X_test = tfidfvect.transform(newsgroups_test.data)\n",
        "y_test = newsgroups_test.target\n",
        "y_pred =  clf.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UkGJhetEPdA4",
        "outputId": "232ee2ce-e904-466e-be57-babc1f319029"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.5854345727938506"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# el F1-score es una metrica adecuada para reportar desempeño de modelos de claificación\n",
        "# es robusta al desbalance de clases. El promediado 'macro' es el promedio de los\n",
        "# F1-score de cada clase. El promedio 'micro' es equivalente a la accuracy que no\n",
        "# es una buena métrica cuando los datasets son desbalanceados\n",
        "f1_score(y_test, y_pred, average='macro')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "McArD4rSDR2K"
      },
      "source": [
        "### Consigna del desafío 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lJgf6GQIIEH1"
      },
      "source": [
        "**1**. Vectorizar documentos. Tomar 5 documentos al azar y medir similaridad con el resto de los documentos.\n",
        "Estudiar los 5 documentos más similares de cada uno analizar si tiene sentido\n",
        "la similaridad según el contenido del texto y la etiqueta de clasificación.\n",
        "\n",
        "**2**. Entrenar modelos de clasificación Naïve Bayes para maximizar el desempeño de clasificación\n",
        "(f1-score macro) en el conjunto de datos de test. Considerar cambiar parámteros\n",
        "de instanciación del vectorizador y los modelos y probar modelos de Naïve Bayes Multinomial\n",
        "y ComplementNB.\n",
        "\n",
        "**3**. Transponer la matriz documento-término. De esa manera se obtiene una matriz\n",
        "término-documento que puede ser interpretada como una colección de vectorización de palabras.\n",
        "Estudiar ahora similaridad entre palabras tomando 5 palabras y estudiando sus 5 más similares. **La elección de palabras no debe ser al azar para evitar la aparición de términos poco interpretables, elegirlas \"manualmente\"**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Vectorización y análisis de similaridad entre documentos\n",
        "\n",
        "A continuación, se vectorizan los documentos, se seleccionan 5 documentos al azar y se mide la similaridad de cada uno con el resto del corpus. Para cada documento, se analizan los 5 documentos más similares considerando el contenido y la etiqueta de clasificación."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{10476: array([ 5064,  9623, 10575, 10836,  2350]),\n",
              " 1824: array([9921, 6364, 5509, 2641, 4359]),\n",
              " 409: array([3444, 5799, 5905, 1764, 3364]),\n",
              " 4506: array([4211, 5928, 6224, 5171, 9491]),\n",
              " 4012: array([ 6599, 10644,  7478,  7308, 10792])}"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import random\n",
        "\n",
        "# Seleccionar 5 documentos al azar del conjunto de entrenamiento\n",
        "random.seed(42)\n",
        "random_docs = random.sample(range(X_train.shape[0]), 5)\n",
        "\n",
        "# Para cada documento seleccionado, calcular la similaridad coseno con el resto\n",
        "doc_similarities = {}\n",
        "for idx in random_docs:\n",
        "    cossim = cosine_similarity(X_train[idx], X_train)[0]\n",
        "    # Excluir el propio documento (índice idx)\n",
        "    most_similar = np.argsort(cossim)[::-1][1:6]\n",
        "    doc_similarities[idx] = most_similar\n",
        "\n",
        "# Mostrar los índices de los documentos seleccionados y sus 5 más similares\n",
        "doc_similarities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Documento 1 (índice 10476):\n",
            "Clase: rec.sport.hockey\n",
            "Texto:\n",
            " This is a general question for US readers:\n",
            "\n",
            "How extensive is the playoff coverage down there?  In Canada, it is almost\n",
            "impossible not to watch a series on TV (ie the only two series I have not had\n",
            "an opportunity to watch this year are Wash-NYI and Chi-Stl, the latter because\n",
            "I'm in the wrong time zone!).  We (in Canada) are basically swamped with \n",
            "coverage, and I wonder how many series/games are televised nationally or even\n",
            "locally in the US and how much precedence they take over, say, local new ...\n",
            "\n",
            "Documentos más similares:\n",
            "  Similar 1 (índice 5064): Clase: rec.sport.hockey\n",
            "  Texto:\n",
            "  \n",
            "I only have one comment on this:  You call this a *classic* playoff year\n",
            "and yet you don't include a Chicago-Detroit series.  C'mon, I'm a Boston\n",
            "fan and I even realize that Chicago-Detroit games are THE most exciting\n",
            "games to watch. ...\n",
            "\n",
            "  Similar 2 (índice 9623): Clase: talk.politics.mideast\n",
            "  Texto:\n",
            "  Accounts of Anti-Armenian Human Right Violations in Azerbaijan #012\n",
            "                 Prelude to Current Events in Nagorno-Karabakh\n",
            "\n",
            "        +---------------------------------------------------------+\n",
            "        |                                                         |\n",
            "        |  I saw a naked girl wi ...\n",
            "\n",
            "  Similar 3 (índice 10575): Clase: sci.crypt\n",
            "  Texto:\n",
            "  \n",
            "I am not an expert in the cryptography science, but some basic things\n",
            "seem evident to me, things which this Clinton Clipper do not address.\n",
            "The all pertain to opportunites for abuse, and conclusions based on what\n",
            "I have seen the membership of this group (except for two notable persons)\n",
            "agree to.  I ...\n",
            "\n",
            "  Similar 4 (índice 10836): Clase: alt.atheism\n",
            "  Texto:\n",
            "  Archive-name: atheism/faq\n",
            "Alt-atheism-archive-name: faq\n",
            "Last-modified: 5 April 1993\n",
            "Version: 1.1\n",
            "\n",
            "                    Alt.Atheism Frequently-Asked Questions\n",
            "\n",
            "This file contains responses to articles which occur repeatedly in\n",
            "alt.atheism.  Points covered here are ones which are not covered in the\n",
            "\"In ...\n",
            "\n",
            "  Similar 5 (índice 2350): Clase: sci.crypt\n",
            "  Texto:\n",
            "  Archive-name: net-privacy/part1\n",
            "Last-modified: 1993/3/3\n",
            "Version: 2.1\n",
            "\n",
            "\n",
            "IDENTITY, PRIVACY, and ANONYMITY on the INTERNET\n",
            "================================================\n",
            "\n",
            "(c) 1993 L. Detweiler.  Not for commercial use except by permission\n",
            "from author, otherwise may be freely copied.  Not to be altere ...\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Documento 2 (índice 1824):\n",
            "Clase: comp.sys.mac.hardware\n",
            "Texto:\n",
            " \n",
            "\n",
            "\tI think this kind of comparison is pretty useless in general.  The\n",
            "processor is only good when a good computer is designed around it adn the\n",
            "computer is used in its designed purpose.  Comparing processor speed is\n",
            "pretty dumb because all you have to do is just increase the clock speed\n",
            "to increase speed among other things.\n",
            "\n",
            "\tI mean how can you say a 040 is faster than a 486 without \n",
            "giving is operational conditions?  Can you say the same when \n",
            "you are running a program that uses a lot of transi ...\n",
            "\n",
            "Documentos más similares:\n",
            "  Similar 1 (índice 9921): Clase: comp.sys.mac.hardware\n",
            "  Texto:\n",
            "  dhk@ubbpc.uucp (Dave Kitabjian) writes ...\n",
            "\n",
            "040 486 030 386 020 286\n",
            "\n",
            "\n",
            "060 fastest, then Pentium, with the first versions of the PowerPC\n",
            "somewhere in the vicinity.\n",
            "\n",
            "\n",
            "No.  Computer speed is only partly dependent of processor/clock speed.\n",
            "Memory system speed play a large role as does video system speed ...\n",
            "\n",
            "  Similar 2 (índice 6364): Clase: comp.sys.mac.hardware\n",
            "  Texto:\n",
            "  Well folks, after some thought the answer struck me flat in the face:\n",
            "\n",
            "\"Why would Apple release a Duo Dock with a processor of its own?\"\n",
            "\n",
            "Here's why- People have hounded Apple for a notebook with a 68040 processor\n",
            "in it. Apple can't deliver that right now because the 040 saps too much\n",
            "power, radiate ...\n",
            "\n",
            "  Similar 3 (índice 5509): Clase: comp.sys.mac.hardware\n",
            "  Texto:\n",
            "  rvenkate@ux4.cso.uiuc.edu (Ravikuma Venkateswar) writes ...\n",
            "\n",
            "Benchmarks are for marketing dweebs and CPU envy.  OK, if it will make\n",
            "you happy, the 486 is faster than the 040.  BFD.  Both architectures\n",
            "are nearing then end of their lifetimes.  And especially with the x86\n",
            "architecture: good riddance.\n",
            " ...\n",
            "\n",
            "  Similar 4 (índice 2641): Clase: comp.sys.mac.hardware\n",
            "  Texto:\n",
            "  \n",
            "    I think this is mostly the fault of the people who write up the\n",
            "literature and price lists being confused themselves. Since there are\n",
            "two possible processor configurations and one of the them doesn't have\n",
            "an FPU it does seem to be an option, even though it really isn't.\n",
            "\n",
            "\n",
            "    Well, then allow m ...\n",
            "\n",
            "  Similar 5 (índice 4359): Clase: comp.sys.mac.hardware\n",
            "  Texto:\n",
            "  If you get the Centris 650 with CD configuration, you are getting a Mac with\n",
            "a 68RC040 processor that has built-in math coprocessor support.  My \n",
            "understanding is that the \"optional fpu\" refers to your option of purchasing\n",
            "the Centris 650 4/80 without FPU OR one of the other configurations WITH FPU. ...\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Documento 3 (índice 409):\n",
            "Clase: comp.graphics\n",
            "Texto:\n",
            " I can't fiqure this out.  I have properly compiled pov on a unix machine\n",
            "running SunOS 4.1.3  The problem is that when I run the sample .pov files and\n",
            "use the EXACT same parameters when compiling different .tga outputs.  Some\n",
            "of the .tga's are okay, and other's are unrecognizable by any software. ...\n",
            "\n",
            "Documentos más similares:\n",
            "  Similar 1 (índice 3444): Clase: comp.graphics\n",
            "  Texto:\n",
            "  Hi, I'm just getting into PoVRay and I was wondering if there is a graphic\n",
            "package that outputs .POV files.  Any help would be appreciated.\n",
            "Thanks.\n",
            "\n",
            "Later'ish\n",
            "Craig\n",
            " ...\n",
            "\n",
            "  Similar 2 (índice 5799): Clase: comp.graphics\n",
            "  Texto:\n",
            "  I finally got a 24 bit viewer for my POVRAY generated .TGA files.\n",
            "\n",
            "It was written in C by Sean Malloy and he kindly sent me a copy.  He\n",
            "wrote it for the same purpose, to view .TGA files using his SpeedStar 24.\n",
            "\n",
            "It ONLY works with the SpeedStar 24 and I cannot send copies since it is\n",
            "not my program.  ...\n",
            "\n",
            "  Similar 3 (índice 5905): Clase: comp.graphics\n",
            "  Texto:\n",
            "  \n",
            "Hallo POV-Renderers !\n",
            "I've got a BocaX3 Card. Now I try to get POV displaying True Colors\n",
            "while rendering. I've tried most of the options and UNIVESA-Driver\n",
            "but what happens isn't correct.\n",
            "Can anybody help me ?\n",
            " ...\n",
            "\n",
            "  Similar 4 (índice 1764): Clase: comp.graphics\n",
            "  Texto:\n",
            "  hi guys\n",
            " like all people in this group i'm a fans of fractal and render sw\n",
            " my favourite are fractint pov & 3dstudio 2.0 \n",
            " now listen my ideas\n",
            " i'have just starting now to be able to use 3dstudio quite well\n",
            " so i'm simulating a full animation of a f1 grand prix\n",
            " unfortanatly just some lap(10?)\n",
            " i' m ...\n",
            "\n",
            "  Similar 5 (índice 3364): Clase: comp.graphics\n",
            "  Texto:\n",
            "  Does anyone know of a good way (standard PC application/PD utility) to\n",
            "convert tif/img/tga files into LaserJet III format.  We would also like to\n",
            "do the same, converting to HPGL (HP plotter) files.\n",
            "\n",
            "Please email any response.\n",
            "\n",
            "Is this the correct group?\n",
            "\n",
            "Thanks in advance.  Michael. ...\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Documento 4 (índice 4506):\n",
            "Clase: rec.autos\n",
            "Texto:\n",
            " \n",
            "This does sound good, but I heard it tends to leave more grit, etc in the \n",
            "oil pan.  Also, I've been told to change the old when it's hot before the\n",
            "grit has much time to settle.\n",
            "\n",
            "Any opinions?\n",
            " ...\n",
            "\n",
            "Documentos más similares:\n",
            "  Similar 1 (índice 4211): Clase: rec.motorcycles\n",
            "  Texto:\n",
            "  \n",
            "\n",
            "It's normal for the BMW K bikes to use a little oil in the first few thousand \n",
            "miles.  I don't know why.  I've had three new K bikes, and all three used a\n",
            "bit of oil when new - max maybe .4 quart in first 1000 miles; this soon quits\n",
            "and by the time I had 10,000 miles on them the oil consumption wa ...\n",
            "\n",
            "  Similar 2 (índice 5928): Clase: comp.sys.mac.hardware\n",
            "  Texto:\n",
            "  or\n",
            "there\n",
            "\n",
            "\n",
            "Okay, I guess its time for a quick explanation of Mac sound.\n",
            "\n",
            "The original documentation for the sound hardware (IM-3) documents how to\n",
            "make sound by directly accessing hardware.  Basically, you jam values\n",
            "into all the even bytes from SoundBase to SoundBase+0x170. This was\n",
            "because\n",
            "of how  ...\n",
            "\n",
            "  Similar 3 (índice 6224): Clase: rec.autos\n",
            "  Texto:\n",
            "  Archive-name: rec-autos/part5\n",
            "\n",
            "[this article is one of a pair of articles containing commonly\n",
            "asked automotive questions; the other article contains questions\n",
            "more geared to the automotive enthusiast, and so is not crossposted\n",
            "to misc.consumers.  -- rpw]\n",
            "\n",
            "   [changes as of 14 April 1993: revised bra ...\n",
            "\n",
            "  Similar 4 (índice 5171): Clase: rec.autos\n",
            "  Texto:\n",
            "  \n",
            "If the tire has a leak you should fix it. \n",
            "\n",
            "\n",
            "Doesn't work too well if the engine is hot, its more accurate to check the\n",
            "oil when the engine is cool, i.e. not when you are at a gas station. ...\n",
            "\n",
            "  Similar 5 (índice 9491): Clase: rec.autos\n",
            "  Texto:\n",
            "  My friend brought a subaru SVX recently.  I had drove it for couples times and I\n",
            "think its a great car, esp on snow.  However when she took it to a local Subaru\n",
            "dealer for a oil change, the bill came out to be about 80 dollars.  The dealer\n",
            "told us it is because to change the oil filter on a SVX it i ...\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Documento 5 (índice 4012):\n",
            "Clase: rec.sport.hockey\n",
            "Texto:\n",
            " For those Leaf fans who are concerned, the following players are slated for\n",
            "return on Thursday's Winnipeg-Toronto game :\n",
            "    Peter Zezel, John Cullen\n",
            "\n",
            "  Mark Osborne and Dave Ellett are questionable to return on Thursday. ...\n",
            "\n",
            "Documentos más similares:\n",
            "  Similar 1 (índice 6599): Clase: soc.religion.christian\n",
            "  Texto:\n",
            "  True.\n",
            "\n",
            "Also read 2 Peter 3:16\n",
            "\n",
            "Peter warns that the scriptures are often hard to understand by those who\n",
            "are not learned on the subject. ...\n",
            "\n",
            "  Similar 2 (índice 10644): Clase: rec.sport.hockey\n",
            "  Texto:\n",
            "  In  <1qvos8$r78@cl.msu.>, vergolin@euler.lbs.msu.edu (David Vergolini) writes...\n",
            "\n",
            "There's quite a few Wings fans lurking about here, they just tend\n",
            "to be low key and thoughtful rather than woofers.  I suppose every\n",
            "family must have a Roger Clinton, though.  But remember (to paraphrase\n",
            "one of my favo ...\n",
            "\n",
            "  Similar 3 (índice 7478): Clase: rec.sport.hockey\n",
            "  Texto:\n",
            "  Toronto                          1 1 1--3\n",
            "Detroit                          1 4 1--6\n",
            "First period\n",
            "     1, Detroit, Yzerman 1 (Gallant, Ciccarelli) 4:48.\n",
            "     2, Toronto, Cullen 1 (Clark, Gill) 10:44.\n",
            "Second period\n",
            "     3, Detroit, Sheppard 1 (Probert, Coffey) pp, 5:04.\n",
            "     4, Detroit, Burr 1 (Racine ...\n",
            "\n",
            "  Similar 4 (índice 7308): Clase: rec.sport.hockey\n",
            "  Texto:\n",
            "  Detroit is a very disciplined team.  There's a lot of Europeans\n",
            "in Detroit which would make the game fast, so Toronto would have\n",
            "to slow the game down, which means drawing penalties, as a last\n",
            "resort anyway.  Toronto will be a good team as soon as they get\n",
            "more good players.  Toronto is just an aver ...\n",
            "\n",
            "  Similar 5 (índice 10792): Clase: rec.sport.baseball\n",
            "  Texto:\n",
            "  \n",
            "\n",
            "The tribe will be in town from April 16 to the 19th.\n",
            "There are ALWAYS tickets available! (Though they are playing Toronto,\n",
            "and many Toronto fans make the trip to Cleveland as it is easier to\n",
            "get tickets in Cleveland than in Toronto.  Either way, I seriously\n",
            "doubt they will sell out until the end o ...\n",
            "\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "for i, idx in enumerate(random_docs):\n",
        "    print(f\"\\nDocumento {i+1} (índice {idx}):\")\n",
        "    print(\"Clase:\", newsgroups_train.target_names[y_train[idx]])\n",
        "    print(\"Texto:\\n\", newsgroups_train.data[idx][:500], \"...\\n\")\n",
        "    print(\"Documentos más similares:\")\n",
        "    for j, sim_idx in enumerate(doc_similarities[idx]):\n",
        "        print(f\"  Similar {j+1} (índice {sim_idx}): Clase: {newsgroups_train.target_names[y_train[sim_idx]]}\")\n",
        "        print(\"  Texto:\\n \", newsgroups_train.data[sim_idx][:300], \"...\\n\")\n",
        "    print(\"-\"*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Optimización de modelos Naïve Bayes y vectorizadores\n",
        "\n",
        "Se probarán diferentes configuraciones de vectorizadores (TF-IDF y CountVectorizer) y modelos Naïve Bayes (MultinomialNB y ComplementNB), variando parámetros como ngram_range, uso de stopwords y max_features. Se reportará el mejor f1-score macro obtenido en el conjunto de test y los parámetros asociados."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vectorizer: TF-IDF, Model: MultinomialNB, Params: {'max_features': None, 'ngram_range': (1, 1), 'stop_words': None}, F1-macro: 0.5854\n",
            "Vectorizer: TF-IDF, Model: ComplementNB, Params: {'max_features': None, 'ngram_range': (1, 1), 'stop_words': None}, F1-macro: 0.6930\n",
            "Vectorizer: TF-IDF, Model: MultinomialNB, Params: {'max_features': None, 'ngram_range': (1, 1), 'stop_words': 'english'}, F1-macro: 0.6468\n",
            "Vectorizer: TF-IDF, Model: ComplementNB, Params: {'max_features': None, 'ngram_range': (1, 1), 'stop_words': 'english'}, F1-macro: 0.6936\n",
            "Vectorizer: TF-IDF, Model: MultinomialNB, Params: {'max_features': None, 'ngram_range': (1, 1), 'stop_words': 'english'}, F1-macro: 0.6468\n",
            "Vectorizer: TF-IDF, Model: ComplementNB, Params: {'max_features': None, 'ngram_range': (1, 1), 'stop_words': 'english'}, F1-macro: 0.6936\n",
            "Vectorizer: TF-IDF, Model: MultinomialNB, Params: {'max_features': None, 'ngram_range': (1, 2), 'stop_words': None}, F1-macro: 0.5398\n",
            "Vectorizer: TF-IDF, Model: MultinomialNB, Params: {'max_features': None, 'ngram_range': (1, 2), 'stop_words': None}, F1-macro: 0.5398\n",
            "Vectorizer: TF-IDF, Model: ComplementNB, Params: {'max_features': None, 'ngram_range': (1, 2), 'stop_words': None}, F1-macro: 0.6817\n",
            "Vectorizer: TF-IDF, Model: ComplementNB, Params: {'max_features': None, 'ngram_range': (1, 2), 'stop_words': None}, F1-macro: 0.6817\n",
            "Vectorizer: TF-IDF, Model: MultinomialNB, Params: {'max_features': None, 'ngram_range': (1, 2), 'stop_words': 'english'}, F1-macro: 0.6425\n",
            "Vectorizer: TF-IDF, Model: MultinomialNB, Params: {'max_features': None, 'ngram_range': (1, 2), 'stop_words': 'english'}, F1-macro: 0.6425\n",
            "Vectorizer: TF-IDF, Model: ComplementNB, Params: {'max_features': None, 'ngram_range': (1, 2), 'stop_words': 'english'}, F1-macro: 0.7039\n",
            "Vectorizer: TF-IDF, Model: ComplementNB, Params: {'max_features': None, 'ngram_range': (1, 2), 'stop_words': 'english'}, F1-macro: 0.7039\n",
            "Vectorizer: TF-IDF, Model: MultinomialNB, Params: {'max_features': 5000, 'ngram_range': (1, 1), 'stop_words': None}, F1-macro: 0.6033\n",
            "Vectorizer: TF-IDF, Model: ComplementNB, Params: {'max_features': 5000, 'ngram_range': (1, 1), 'stop_words': None}, F1-macro: 0.6344\n",
            "Vectorizer: TF-IDF, Model: MultinomialNB, Params: {'max_features': 5000, 'ngram_range': (1, 1), 'stop_words': None}, F1-macro: 0.6033\n",
            "Vectorizer: TF-IDF, Model: ComplementNB, Params: {'max_features': 5000, 'ngram_range': (1, 1), 'stop_words': None}, F1-macro: 0.6344\n",
            "Vectorizer: TF-IDF, Model: MultinomialNB, Params: {'max_features': 5000, 'ngram_range': (1, 1), 'stop_words': 'english'}, F1-macro: 0.6251\n",
            "Vectorizer: TF-IDF, Model: ComplementNB, Params: {'max_features': 5000, 'ngram_range': (1, 1), 'stop_words': 'english'}, F1-macro: 0.6351\n",
            "Vectorizer: TF-IDF, Model: MultinomialNB, Params: {'max_features': 5000, 'ngram_range': (1, 1), 'stop_words': 'english'}, F1-macro: 0.6251\n",
            "Vectorizer: TF-IDF, Model: ComplementNB, Params: {'max_features': 5000, 'ngram_range': (1, 1), 'stop_words': 'english'}, F1-macro: 0.6351\n",
            "Vectorizer: TF-IDF, Model: MultinomialNB, Params: {'max_features': 5000, 'ngram_range': (1, 2), 'stop_words': None}, F1-macro: 0.5657\n",
            "Vectorizer: TF-IDF, Model: ComplementNB, Params: {'max_features': 5000, 'ngram_range': (1, 2), 'stop_words': None}, F1-macro: 0.5937\n",
            "Vectorizer: TF-IDF, Model: MultinomialNB, Params: {'max_features': 5000, 'ngram_range': (1, 2), 'stop_words': None}, F1-macro: 0.5657\n",
            "Vectorizer: TF-IDF, Model: ComplementNB, Params: {'max_features': 5000, 'ngram_range': (1, 2), 'stop_words': None}, F1-macro: 0.5937\n",
            "Vectorizer: TF-IDF, Model: MultinomialNB, Params: {'max_features': 5000, 'ngram_range': (1, 2), 'stop_words': 'english'}, F1-macro: 0.6209\n",
            "Vectorizer: TF-IDF, Model: ComplementNB, Params: {'max_features': 5000, 'ngram_range': (1, 2), 'stop_words': 'english'}, F1-macro: 0.6303\n",
            "Vectorizer: TF-IDF, Model: MultinomialNB, Params: {'max_features': 5000, 'ngram_range': (1, 2), 'stop_words': 'english'}, F1-macro: 0.6209\n",
            "Vectorizer: TF-IDF, Model: ComplementNB, Params: {'max_features': 5000, 'ngram_range': (1, 2), 'stop_words': 'english'}, F1-macro: 0.6303\n",
            "Vectorizer: Count, Model: MultinomialNB, Params: {'max_features': None, 'ngram_range': (1, 1), 'stop_words': None}, F1-macro: 0.5121\n",
            "Vectorizer: Count, Model: ComplementNB, Params: {'max_features': None, 'ngram_range': (1, 1), 'stop_words': None}, F1-macro: 0.6374\n",
            "Vectorizer: Count, Model: MultinomialNB, Params: {'max_features': None, 'ngram_range': (1, 1), 'stop_words': None}, F1-macro: 0.5121\n",
            "Vectorizer: Count, Model: ComplementNB, Params: {'max_features': None, 'ngram_range': (1, 1), 'stop_words': None}, F1-macro: 0.6374\n",
            "Vectorizer: Count, Model: MultinomialNB, Params: {'max_features': None, 'ngram_range': (1, 1), 'stop_words': 'english'}, F1-macro: 0.5968\n",
            "Vectorizer: Count, Model: ComplementNB, Params: {'max_features': None, 'ngram_range': (1, 1), 'stop_words': 'english'}, F1-macro: 0.6433\n",
            "Vectorizer: Count, Model: MultinomialNB, Params: {'max_features': None, 'ngram_range': (1, 1), 'stop_words': 'english'}, F1-macro: 0.5968\n",
            "Vectorizer: Count, Model: ComplementNB, Params: {'max_features': None, 'ngram_range': (1, 1), 'stop_words': 'english'}, F1-macro: 0.6433\n",
            "Vectorizer: Count, Model: MultinomialNB, Params: {'max_features': None, 'ngram_range': (1, 2), 'stop_words': None}, F1-macro: 0.3995\n",
            "Vectorizer: Count, Model: MultinomialNB, Params: {'max_features': None, 'ngram_range': (1, 2), 'stop_words': None}, F1-macro: 0.3995\n",
            "Vectorizer: Count, Model: ComplementNB, Params: {'max_features': None, 'ngram_range': (1, 2), 'stop_words': None}, F1-macro: 0.6588\n",
            "Vectorizer: Count, Model: ComplementNB, Params: {'max_features': None, 'ngram_range': (1, 2), 'stop_words': None}, F1-macro: 0.6588\n",
            "Vectorizer: Count, Model: MultinomialNB, Params: {'max_features': None, 'ngram_range': (1, 2), 'stop_words': 'english'}, F1-macro: 0.5813\n",
            "Vectorizer: Count, Model: MultinomialNB, Params: {'max_features': None, 'ngram_range': (1, 2), 'stop_words': 'english'}, F1-macro: 0.5813\n",
            "Vectorizer: Count, Model: ComplementNB, Params: {'max_features': None, 'ngram_range': (1, 2), 'stop_words': 'english'}, F1-macro: 0.6712\n",
            "Vectorizer: Count, Model: ComplementNB, Params: {'max_features': None, 'ngram_range': (1, 2), 'stop_words': 'english'}, F1-macro: 0.6712\n",
            "Vectorizer: Count, Model: MultinomialNB, Params: {'max_features': 5000, 'ngram_range': (1, 1), 'stop_words': None}, F1-macro: 0.5561\n",
            "Vectorizer: Count, Model: ComplementNB, Params: {'max_features': 5000, 'ngram_range': (1, 1), 'stop_words': None}, F1-macro: 0.5540\n",
            "Vectorizer: Count, Model: MultinomialNB, Params: {'max_features': 5000, 'ngram_range': (1, 1), 'stop_words': None}, F1-macro: 0.5561\n",
            "Vectorizer: Count, Model: ComplementNB, Params: {'max_features': 5000, 'ngram_range': (1, 1), 'stop_words': None}, F1-macro: 0.5540\n",
            "Vectorizer: Count, Model: MultinomialNB, Params: {'max_features': 5000, 'ngram_range': (1, 1), 'stop_words': 'english'}, F1-macro: 0.5709\n",
            "Vectorizer: Count, Model: ComplementNB, Params: {'max_features': 5000, 'ngram_range': (1, 1), 'stop_words': 'english'}, F1-macro: 0.5644\n",
            "Vectorizer: Count, Model: MultinomialNB, Params: {'max_features': 5000, 'ngram_range': (1, 1), 'stop_words': 'english'}, F1-macro: 0.5709\n",
            "Vectorizer: Count, Model: ComplementNB, Params: {'max_features': 5000, 'ngram_range': (1, 1), 'stop_words': 'english'}, F1-macro: 0.5644\n",
            "Vectorizer: Count, Model: MultinomialNB, Params: {'max_features': 5000, 'ngram_range': (1, 2), 'stop_words': None}, F1-macro: 0.5070\n",
            "Vectorizer: Count, Model: ComplementNB, Params: {'max_features': 5000, 'ngram_range': (1, 2), 'stop_words': None}, F1-macro: 0.5061\n",
            "Vectorizer: Count, Model: MultinomialNB, Params: {'max_features': 5000, 'ngram_range': (1, 2), 'stop_words': None}, F1-macro: 0.5070\n",
            "Vectorizer: Count, Model: ComplementNB, Params: {'max_features': 5000, 'ngram_range': (1, 2), 'stop_words': None}, F1-macro: 0.5061\n",
            "Vectorizer: Count, Model: MultinomialNB, Params: {'max_features': 5000, 'ngram_range': (1, 2), 'stop_words': 'english'}, F1-macro: 0.5648\n",
            "Vectorizer: Count, Model: ComplementNB, Params: {'max_features': 5000, 'ngram_range': (1, 2), 'stop_words': 'english'}, F1-macro: 0.5532\n",
            "\n",
            "Mejor configuración:\n",
            "{'vectorizer': 'TF-IDF', 'ngram_range': (1, 2), 'stop_words': 'english', 'max_features': None, 'model': 'ComplementNB'}\n",
            "Mejor F1-score macro: 0.7039\n",
            "Vectorizer: Count, Model: MultinomialNB, Params: {'max_features': 5000, 'ngram_range': (1, 2), 'stop_words': 'english'}, F1-macro: 0.5648\n",
            "Vectorizer: Count, Model: ComplementNB, Params: {'max_features': 5000, 'ngram_range': (1, 2), 'stop_words': 'english'}, F1-macro: 0.5532\n",
            "\n",
            "Mejor configuración:\n",
            "{'vectorizer': 'TF-IDF', 'ngram_range': (1, 2), 'stop_words': 'english', 'max_features': None, 'model': 'ComplementNB'}\n",
            "Mejor F1-score macro: 0.7039\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import ParameterGrid\n",
        "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
        "\n",
        "vectorizer_options = [\n",
        "    {'vectorizer': TfidfVectorizer, 'name': 'TF-IDF'},\n",
        "    {'vectorizer': CountVectorizer, 'name': 'Count'}\n",
        "]\n",
        "\n",
        "param_grid = {\n",
        "    'ngram_range': [(1,1), (1,2)],\n",
        "    'stop_words': [None, 'english'],\n",
        "    'max_features': [None, 5000]\n",
        "}\n",
        "\n",
        "models = [\n",
        "    {'model': MultinomialNB, 'name': 'MultinomialNB'},\n",
        "    {'model': ComplementNB, 'name': 'ComplementNB'}\n",
        "]\n",
        "\n",
        "best_score = 0\n",
        "best_config = None\n",
        "\n",
        "for vect_opt in vectorizer_options:\n",
        "    for params in ParameterGrid(param_grid):\n",
        "        vect = vect_opt['vectorizer'](\n",
        "            ngram_range=params['ngram_range'],\n",
        "            stop_words=params['stop_words'],\n",
        "            max_features=params['max_features']\n",
        "        )\n",
        "        X_train_vect = vect.fit_transform(newsgroups_train.data)\n",
        "        X_test_vect = vect.transform(newsgroups_test.data)\n",
        "        for model_opt in models:\n",
        "            clf = model_opt['model']()\n",
        "            clf.fit(X_train_vect, y_train)\n",
        "            y_pred = clf.predict(X_test_vect)\n",
        "            score = f1_score(y_test, y_pred, average='macro')\n",
        "            if score > best_score:\n",
        "                best_score = score\n",
        "                best_config = {\n",
        "                    'vectorizer': vect_opt['name'],\n",
        "                    'ngram_range': params['ngram_range'],\n",
        "                    'stop_words': params['stop_words'],\n",
        "                    'max_features': params['max_features'],\n",
        "                    'model': model_opt['name']\n",
        "                }\n",
        "            print(f\"Vectorizer: {vect_opt['name']}, Model: {model_opt['name']}, Params: {params}, F1-macro: {score:.4f}\")\n",
        "\n",
        "print(\"\\nMejor configuración:\")\n",
        "print(best_config)\n",
        "print(f\"Mejor F1-score macro: {best_score:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Similaridad entre palabras a partir de la matriz término-documento\n",
        "\n",
        "Se transpone la matriz documento-término para obtener una matriz término-documento, donde cada fila representa un vector de una palabra en el espacio de los documentos. Se seleccionan 5 palabras relevantes manualmente y se calcula la similaridad coseno entre sus vectores y los del resto del vocabulario. Se muestran las 5 palabras más similares para cada una."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Palabra: 'car'\n",
            "Palabras más similares:\n",
            "  cars (similitud: 0.180)\n",
            "  criterium (similitud: 0.177)\n",
            "  civic (similitud: 0.175)\n",
            "  owner (similitud: 0.169)\n",
            "  dealer (similitud: 0.168)\n",
            "\n",
            "Palabra: 'god'\n",
            "Palabras más similares:\n",
            "  jesus (similitud: 0.269)\n",
            "  bible (similitud: 0.262)\n",
            "  that (similitud: 0.256)\n",
            "  existence (similitud: 0.255)\n",
            "  christ (similitud: 0.251)\n",
            "\n",
            "Palabra: 'computer'\n",
            "Palabras más similares:\n",
            "  decwriter (similitud: 0.156)\n",
            "  harkens (similitud: 0.152)\n",
            "  deluged (similitud: 0.152)\n",
            "  shopper (similitud: 0.144)\n",
            "  the (similitud: 0.136)\n",
            "\n",
            "Palabra: 'space'\n",
            "Palabras más similares:\n",
            "  nasa (similitud: 0.330)\n",
            "  seds (similitud: 0.297)\n",
            "  shuttle (similitud: 0.293)\n",
            "  enfant (similitud: 0.280)\n",
            "  seti (similitud: 0.246)\n",
            "\n",
            "Palabra: 'music'\n",
            "Palabras más similares:\n",
            "  inconveniences (similitud: 0.415)\n",
            "  colect (similitud: 0.415)\n",
            "  posses (similitud: 0.415)\n",
            "  deaf (similitud: 0.408)\n",
            "  competencies (similitud: 0.402)\n"
          ]
        }
      ],
      "source": [
        "# Transponer la matriz documento-término para obtener término-documento\n",
        "X_terms = X_train.transpose()\n",
        "\n",
        "# Seleccionar 5 palabras relevantes manualmente (asegúrate de que existan en el vocabulario)\n",
        "selected_words = ['car', 'god', 'computer', 'space', 'music']\n",
        "\n",
        "# Obtener los índices de las palabras seleccionadas\n",
        "token2idx = tfidfvect.vocabulary_\n",
        "idx2token = {v: k for k, v in token2idx.items()}\n",
        "selected_indices = [token2idx[w] for w in selected_words if w in token2idx]\n",
        "\n",
        "for idx in selected_indices:\n",
        "    word = idx2token[idx]\n",
        "    # Similaridad coseno entre la palabra y todas las demás\n",
        "    cossim = cosine_similarity(X_terms[idx], X_terms)[0]\n",
        "    # Excluir la propia palabra\n",
        "    most_similar = np.argsort(cossim)[::-1][1:6]\n",
        "    print(f\"\\nPalabra: '{word}'\")\n",
        "    print(\"Palabras más similares:\")\n",
        "    for sim_idx in most_similar:\n",
        "        print(f\"  {idx2token[sim_idx]} (similitud: {cossim[sim_idx]:.3f})\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Posgrado-AI",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
